{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "039809df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import emoji\n",
    "import jieba\n",
    "import jieba.posseg as pseg #这个包可以标注词性，我们只需要n,v,a\n",
    "from zhconv import convert\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b2283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3ae28e0",
   "metadata": {},
   "source": [
    "#### 单平台数据初步预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7724ba31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zhihu Addition Set Shape = (101694, 9)\n",
      "Zhihu Addition Set Memory Usage = 6.98 MB\n"
     ]
    }
   ],
   "source": [
    "df_zhihu = pd.read_csv('../ITP/data_zhihu_addition.csv')\n",
    "df_zhihu['source'] = 'zhihu'\n",
    "print(f'Zhihu Addition Set Shape = {df_zhihu.shape}')  \n",
    "print('Zhihu Addition Set Memory Usage = {:.2f} MB'.format(df_zhihu.memory_usage().sum() / 1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f13e1e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>fans_count</th>\n",
       "      <th>content</th>\n",
       "      <th>created_time</th>\n",
       "      <th>updated_time</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>voteup_count</th>\n",
       "      <th>url</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>一只可尼熊</td>\n",
       "      <td>12</td>\n",
       "      <td>不建议  图情不是重点院校的话  很水</td>\n",
       "      <td>2019-08-31 15:33:32</td>\n",
       "      <td>2019-08-31 15:33:32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.zhihu.com/api/v4//answers/809074756</td>\n",
       "      <td>zhihu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>知乎用户</td>\n",
       "      <td>71</td>\n",
       "      <td>在某些領域相當一部份都是華人，而且內地高校有些水平也不錯，拿過頂會best paper（比如...</td>\n",
       "      <td>2014-12-04 11:10:27</td>\n",
       "      <td>2015-01-06 16:25:50</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.zhihu.com/api/v4//answers/34518749</td>\n",
       "      <td>zhihu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chris</td>\n",
       "      <td>73</td>\n",
       "      <td>题主问的是中国AI水平，下面一堆人回答华人AI水平，他们代表的很多是美国的水平吧？</td>\n",
       "      <td>2017-08-25 08:24:07</td>\n",
       "      <td>2017-08-25 08:24:07</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.zhihu.com/api/v4//answers/220067758</td>\n",
       "      <td>zhihu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>知乎用户</td>\n",
       "      <td>120</td>\n",
       "      <td>这个说来也挺奇怪的。其实在美帝很多高校做这块的都是华人，甚至可以放宽一些，计算机系里面华人比...</td>\n",
       "      <td>2014-12-06 18:44:39</td>\n",
       "      <td>2014-12-06 18:44:39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.zhihu.com/api/v4//answers/34657913</td>\n",
       "      <td>zhihu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>知乎用户</td>\n",
       "      <td>55</td>\n",
       "      <td>这问题让人如何答好吧，不错的水平</td>\n",
       "      <td>2014-12-09 08:22:52</td>\n",
       "      <td>2014-12-09 08:22:52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.zhihu.com/api/v4//answers/34797357</td>\n",
       "      <td>zhihu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author  fans_count                                            content  \\\n",
       "0  一只可尼熊          12                                不建议  图情不是重点院校的话  很水   \n",
       "1   知乎用户          71  在某些領域相當一部份都是華人，而且內地高校有些水平也不錯，拿過頂會best paper（比如...   \n",
       "2  Chris          73          题主问的是中国AI水平，下面一堆人回答华人AI水平，他们代表的很多是美国的水平吧？   \n",
       "3   知乎用户         120  这个说来也挺奇怪的。其实在美帝很多高校做这块的都是华人，甚至可以放宽一些，计算机系里面华人比...   \n",
       "4   知乎用户          55                                   这问题让人如何答好吧，不错的水平   \n",
       "\n",
       "          created_time         updated_time  comment_count  voteup_count  \\\n",
       "0  2019-08-31 15:33:32  2019-08-31 15:33:32              0             0   \n",
       "1  2014-12-04 11:10:27  2015-01-06 16:25:50              1             2   \n",
       "2  2017-08-25 08:24:07  2017-08-25 08:24:07              0             2   \n",
       "3  2014-12-06 18:44:39  2014-12-06 18:44:39              1             1   \n",
       "4  2014-12-09 08:22:52  2014-12-09 08:22:52              0             0   \n",
       "\n",
       "                                               url source  \n",
       "0  https://www.zhihu.com/api/v4//answers/809074756  zhihu  \n",
       "1   https://www.zhihu.com/api/v4//answers/34518749  zhihu  \n",
       "2  https://www.zhihu.com/api/v4//answers/220067758  zhihu  \n",
       "3   https://www.zhihu.com/api/v4//answers/34657913  zhihu  \n",
       "4   https://www.zhihu.com/api/v4//answers/34797357  zhihu  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zhihu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4f284792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df_zhihu = df_zhihu.rename(columns={'content':'comments','updated_time':'time'})\n",
    "\n",
    "# convert data type\n",
    "df_zhihu['time'] = df_zhihu['time'].astype('datetime64[ns]')\n",
    "\n",
    "# extract data(source, topics, comments, time)\n",
    "df_zhihu = df_zhihu[['source','comments','time']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55649997",
   "metadata": {},
   "source": [
    "Merge data from four platforms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f6f2f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge data from four platforms\n",
    "df = df_zhihu.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711a54a3",
   "metadata": {},
   "source": [
    "#### 规范化预处理（Cleansing）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fe3415c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size =  (101694, 3)\n",
      "After drop duplicated data size =  (98561, 3)\n",
      "After drop missing data size =  (98560, 3)\n"
     ]
    }
   ],
   "source": [
    "# drop duplicated\n",
    "print('Data size = ',df.shape)\n",
    "df = df.drop_duplicates(subset='comments')\n",
    "# print('Duplicates data size = ',df.duplicated(subset='comments').sum())\n",
    "print('After drop duplicated data size = ',df.shape)\n",
    "\n",
    "# drop data missing time or comments\n",
    "df.dropna(subset=['comments'],inplace=True)\n",
    "df.dropna(subset=['time'],inplace=True)\n",
    "print('After drop missing data size = ',df.shape)\n",
    "df.index = range(0,len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "398e6bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data clean\n",
    "def clean(text):\n",
    "    '''\n",
    "    正则化\n",
    "    '''\n",
    "    \n",
    "    # 清除@和回复/转发中的用户名\n",
    "    text = re.sub(r\"(回复)?(//)?\\s*@\\S*?\\s*(:| |$)\", \"\", text).strip()\n",
    "    \n",
    "    # 清除emoji表情\n",
    "    text = emoji.demojize(text).strip()   \n",
    "    \n",
    "    # 清除##话题内容\n",
    "    text = re.sub(r\"#\\S+#\", \"\", text).strip()    \n",
    "    \n",
    "    # 清除网址link\n",
    "    URL_REGEX1 = re.compile('</?\\w+[^>]*>')\n",
    "    URL_REGEX2 = re.compile('(https|http|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]')\n",
    "    text = re.sub(URL_REGEX1, \"\", text).strip()\n",
    "    text = re.sub(URL_REGEX2, \"\", text).strip()\n",
    "    \n",
    "    # 清除数字\n",
    "    text = re.sub('[\\d]+','',text).strip()\n",
    "    \n",
    "    # 清除字母\n",
    "    text = re.sub('[a-zA-Z]+','',text).strip()\n",
    "    \n",
    "    # 清除无意义的词语\n",
    "    text = text.replace(\"转发微博\", \"\").strip()\n",
    "    \n",
    "    # 合并多余的空格\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip() \n",
    "    \n",
    "    # 清除标点\n",
    "    r = \"[_.+-=——$%^~@#￥%……&*《》<>「」{}【】()“”\\\"/]+\"\n",
    "    text = re.sub(r, '', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "547ff2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(text):\n",
    "    try:\n",
    "        return convert(text,'zh-hans')\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "20bb661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 98560/98560 [00:11<00:00, 8236.61it/s]\n",
      "100%|███████████████████████████████████| 98560/98560 [00:10<00:00, 9737.37it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['comments'] = df['comments'].progress_apply(clean) # 综合正则化处理\n",
    "df['comments'] = df['comments'].progress_apply(simplify) # 繁体字简体化\n",
    "# df['topics'] = df['topics'].progress_apply(clean) # 繁体字简体化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ba152a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size =  (97906, 3)\n",
      "After drop duplicated data size =  (97906, 3)\n"
     ]
    }
   ],
   "source": [
    "# drop duplicated\n",
    "print('Data size = ',df2.shape)\n",
    "df2.drop_duplicates(subset='comments',inplace=True)\n",
    "print('After drop duplicated data size = ',df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6282e042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acec82d2",
   "metadata": {},
   "source": [
    "#### 数据过滤预处理（overview_data）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "49cc37c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file):\n",
    "    #获取文件最可能的编码格式\n",
    "    import chardet\n",
    "    with open(file, \"rb\") as f:\n",
    "        r = f.read()\n",
    "    e = chardet.detect(r)\n",
    "    encoding = e.get(\"encoding\")\n",
    "    return encoding\n",
    "\n",
    "def get_stopwords(stop_words_file, encoding):\n",
    "    \"\"\"\n",
    "    读取停用词文件\n",
    "    参数：\n",
    "        停用词文件\n",
    "    返回：\n",
    "        停用词列表\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(stop_words_file,encoding = encoding) as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    "    return custom_stopwords_list\n",
    "\n",
    "def get_tokenization(text):\n",
    "    \"\"\"\n",
    "    使用jieba分词并剔除停用词\n",
    "    参数：\n",
    "        文本字符串\n",
    "    返回：\n",
    "        以空格连接的词串\n",
    "    \"\"\"\n",
    "    word_token = []\n",
    "    for word in jieba.lcut(text.strip(), cut_all=False): #精确模式\n",
    "        if word not in STOPWORDS:\n",
    "            word_token.append(word)\n",
    "    return ' '.join(word_token)\n",
    "\n",
    "def clear_singlechar(text):\n",
    "    \"\"\"\n",
    "    清除切词后的单个字符\n",
    "    \"\"\"\n",
    "    word_cleared = []\n",
    "    for word in text.strip().split(r' '):\n",
    "        if len(word)!=1:\n",
    "            word_cleared.append(word)\n",
    "    return ' '.join(word_cleared)\n",
    "\n",
    "def pos_tag_filter_nva(text):\n",
    "    finance_noisy = [\"疫情\", \"股市\", \"风险投资\", \"风险融资\", \"风险资本\", \"信贷风险\", \"贷款风险\", \"投资风险\", \"交易风险\", \"信用风险\", \"风险评分\", \"金融风险\", \"基金\", \"个股\"]\n",
    "    \n",
    "    words = pseg.cut(text)\n",
    "    filtered_words = []\n",
    "    for word, flag in words:\n",
    "        # a = (\"{0}/{1}\".format(word, flag))\n",
    "        if (flag == 'n'or flag == 'v' or flag == 'a') and (len(word) > 1) and word not in finance_noisy:\n",
    "            filtered_words.append(word)\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0af1f647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words = 2576\n"
     ]
    }
   ],
   "source": [
    "# 停用词表\n",
    "stop_qua = r'ー、一、二、三、四、五、一点、两个、三个、一些、一种、几个、几种；和、跟、与、既、同、及、而、况、况且、何况、乃至、则、乃、就、而、便、于是、然后、至于、说到、此外、像、如、一般、比方、接着、却、虽然、但是、然而、而、偏偏、只是、不过、至于、致、不料、岂知、原来、因为、由于、以便、因此、所以、是故、以致、或、或者、还是、抑、非…即、不是…就是、若、如果、若是、假如、只要、除非、假使、倘若、即使、假若、要是、譬如、像、好比、如同、似乎、等于；不如、不及、与其…不如、若…则、虽然…可是、虽然、固然、尽管、纵然、即使、不但、不仅、而且、何况、并、且、不管、只要、除非、以、以便、以免、为了'\n",
    "\n",
    "STOPWORDS = list(set(get_stopwords('../ITP/Data collection/stopwordsHIT.txt', encoding=detect_encoding('../ITP/Data collection/stopwordsHIT.txt')) + get_stopwords(r'../ITP/Data collection/stopwords.txt', encoding='gbk') + stop_qua.split(r'、')))\n",
    "print(f'Number of stop words = {len(STOPWORDS)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b2f05c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/29/27cgqwrx7_18wcd66rpffzlc0000gn/T/jieba.cache\n",
      "Loading model cost 0.367 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|████████████████████████████████████| 98560/98560 [07:46<00:00, 211.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# 切词\n",
    "df = df.copy()\n",
    "jieba.load_userdict('../ITP/Data collection/dict.txt')\n",
    "\n",
    "tqdm.pandas()\n",
    "df['tokenization'] = df['comments'].progress_apply(get_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "68e72cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 98560/98560 [00:01<00:00, 94737.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# 清除单个字符\n",
    "df = df.copy()\n",
    "tqdm.pandas()\n",
    "df['tokenization'] = df['tokenization'].progress_apply(clear_singlechar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fa6f02dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 98560/98560 [03:06<00:00, 529.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# 过滤切词结果，保留n/v/adj\n",
    "df = df.copy()\n",
    "jieba.load_userdict('../ITP/Data collection/dict.txt')\n",
    "\n",
    "tqdm.pandas()\n",
    "df['tokenization_filtered'] = df['tokenization'].progress_apply(pos_tag_filter_nva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c171dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3fe4b22",
   "metadata": {},
   "source": [
    "#### 对微博含有广告、打榜等重复噪音数据的补充处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b76166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1.根据关键词对微博数据再做初筛 (主要针对微博数据)\n",
    "# words_list = ['风险','安全隐患','威胁','危险','危机','后果','危害','安全事件','预防','安全事故','风险管理','负面影响','灾害','困境','欺诈','失误','漏洞']\n",
    "# risk_words = str()\n",
    "# for item in words_list:\n",
    "#     risk_words = risk_words+str(item)+'|'\n",
    "    \n",
    "# # 筛掉所有不含有“人工智能”和“risk_words”中关键词的内容\n",
    "# data_weibo_filtered = data_weibo[(data_weibo['tokenization_filtered'].str.contains(\"人工智能\"))==True]\n",
    "\n",
    "# # 筛掉所有不含有“人工智能”和“words_list”中关键词的内容\n",
    "# data_weibo_filtered= data_weibo_filtered[(data_weibo_filtered['tokenization_filtered'].str.contains(risk_words))==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2.使用fuzzywuzzy库，对数据进行遍历，剔除掉相似度75%以上的数据（主要针对微博数据）\n",
    "\n",
    "# # Final Version (itertools.islice)\n",
    "# def Similarity(index_tuples_list):\n",
    "#     res = []\n",
    "# #     print(index_tuples_list)\n",
    "#     for p in index_tuples_list:\n",
    "#         similarity = fuzz.ratio(data_weibo_filtered['comments'][p[0]],data_weibo_filtered['comments'][p[1]])\n",
    "#         if similarity > 75:\n",
    "#             res.append([p[0],p[1],similarity])\n",
    "#     return res\n",
    "\n",
    "# pair_similarity = []\n",
    "# begin_all = time.time()\n",
    "\n",
    "# obj_list = []\n",
    "# executor = ThreadPoolExecutor(max_workers=8)\n",
    "\n",
    "# # 分批次 1000000000*18\n",
    "# for i in tqdm(range(3000000000,4000000000,5000000),position=0, leave=True):\n",
    "#     obj_list.append(itertools.islice(itertools.combinations(data_weibo_filtered.index.to_list(),2),i,i+5000000)) \n",
    "\n",
    "# for res in executor.map(Similarity, obj_list):\n",
    "#     if (len(res)) > 0:\n",
    "#         pair_similarity.extend(res)\n",
    "    \n",
    "# end_all = time.time()\n",
    "# print(end_all-begin_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "338103d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98560, 5)\n",
      "(98560, 5)\n"
     ]
    }
   ],
   "source": [
    "# 3.删除分词过滤后无词的数据\n",
    "print(df.shape)\n",
    "df.dropna(subset=['tokenization_filtered'], inplace=True)\n",
    "# df.reset_index(inplace=True, drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3626dd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 98560/98560 [00:00<00:00, 447467.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71700, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.删除分词+过滤后词数<5的数据(可选操作)\n",
    "'''\n",
    "item_word_below_5 = []\n",
    "\n",
    "tqdm.pandas()\n",
    "df['word_count'] = df['tokenization_filtered'].progress_apply(lambda x: len(x.split()))\n",
    "df = df[df['word_count']>=5][['source','comments','time','tokenization','tokenization_filtered','word_count']]# ,'topics'\n",
    "print(df.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a8fa161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['source','comments','time','tokenization','tokenization_filtered']].to_csv('data_zhihu_addition_filtered.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe71ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
