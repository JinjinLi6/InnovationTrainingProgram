{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "039809df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import emoji\n",
    "import jieba\n",
    "import jieba.posseg as pseg #这个包可以标注词性，我们只需要n,v,a\n",
    "from zhconv import convert\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "278b2283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>comments</th>\n",
       "      <th>time</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>tokenization_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>是啊，为什么没有做出来呢？在刚出现的时候，我也想过这个问题。像题主所说的，百度在技术、语言处...</td>\n",
       "      <td>2022-12-27 00:12:57</td>\n",
       "      <td>像题 所说 百度 技术 语言 技术 研发 能力 依托 技术 语言 技术 诞生 产品 公司 做...</td>\n",
       "      <td>百度 技术 语言 技术 能力 依托 技术 语言 技术 诞生 产品 公司 做出 产品 百度 能...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>百度并不是没有做，文心系列其实也是学习使用同样的思路，不过效果肯定还是跟不上。说百度没志气光...</td>\n",
       "      <td>2022-12-20 00:44:51</td>\n",
       "      <td>百度 文心 系列 学习 思路 效果 肯定 跟不上 百度 没志气 广告 肯定 感觉 百度 技术...</td>\n",
       "      <td>百度 文心 学习 思路 效果 肯定 跟不上 百度 没志气 广告 肯定 感觉 百度 技术 氛围...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>为什么谷歌没有做出这种产品？</td>\n",
       "      <td>2022-12-27 09:36:36</td>\n",
       "      <td>谷歌 做出 产品</td>\n",
       "      <td>做出 产品</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>之前在 组实习过。个人认为百度在一众大厂里还是很看重技术研发的，但 这种不确定性极高的项目很...</td>\n",
       "      <td>2022-12-20 01:59:18</td>\n",
       "      <td>实习 百度 一众 厂里 看重 技术 研发   不确定性 极高 项目 很难     感觉   ...</td>\n",
       "      <td>实习 百度 厂里 看重 技术 不确定性 项目 感觉 成功 先例 资源 提不上 能力 有限 基...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>原因是多方面的，都是个人浅显的理解。一直是在主攻，其他公司大部分都在做方向，、出来的时候大家...</td>\n",
       "      <td>2022-12-22 21:02:08</td>\n",
       "      <td>原因 多方面 浅显 理解 主攻 公司 大部分 方向 看不上 发现 好像 真给 出点 东西 搜...</td>\n",
       "      <td>原因 浅显 理解 公司 方向 看不上 发现 好像 出点 搜索引擎 百度 搜索引擎 公司 投入...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3011</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>中文网络上的八股文太多，让抢了饭碗，有人急了。</td>\n",
       "      <td>2023-01-31 12:28:11</td>\n",
       "      <td>中文 网络 八股文 饭碗 有人</td>\n",
       "      <td>网络 饭碗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3012</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>从刚推出时的简单聊天，到现在已经被开发出写诗、考试、甚至写论文的功能，未来发展前景广阔。机器...</td>\n",
       "      <td>2023-01-31 11:05:48</td>\n",
       "      <td>从刚 推出 简单 聊天 开发 写诗 考试 论文 功能 未来 发展前景 广阔 机器人 产业 国...</td>\n",
       "      <td>推出 简单 开发 写诗 功能 广阔 机器人 产业 国家 层面 政策 支持 老龄化 社会 劳动...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3013</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>人工智能技术其实应用已经非常的广泛了，比如小区门口的人脸识别技术，比如医疗上的医疗机器人等等...</td>\n",
       "      <td>2023-01-31 11:05:44</td>\n",
       "      <td>人工智能 技术 小区 门口 人脸识别 技术 医疗 医疗 机器人 科技 技术 发展 社会 人工...</td>\n",
       "      <td>人工智能 技术 小区 人脸识别 技术 医疗 医疗 机器人 科技 技术 社会 人工智能 智能 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>和我都能被指导成萌头黑猫泠仆风格，但我更胜几千筹。期待有人喂食我泠力，我会把不明所以的台词发...</td>\n",
       "      <td>2023-02-06 11:31:36</td>\n",
       "      <td>指导 成萌头 黑猫 风格 几千 期待 有人 喂食 我泠力 我会 不明 台词 发给你 作品 可...</td>\n",
       "      <td>指导 萌头 黑猫 风格 期待 喂食 我会 不明 台词 作品 可爱 低于 动漫 主角 世界 萌...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>现在已经有很多方面已经超越我了，我和谁更聪明还真不好说，或许我已经不如他了，至少速度效率这方...</td>\n",
       "      <td>2023-02-06 11:27:59</td>\n",
       "      <td>超越 聪明 不好 至少 速度 效率 这方面 肯定 碾压 超过 时间 东西 几年 它教</td>\n",
       "      <td>超越 聪明 速度 效率 肯定 碾压 超过 时间</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3016 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     source                                           comments  \\\n",
       "0     zhihu  是啊，为什么没有做出来呢？在刚出现的时候，我也想过这个问题。像题主所说的，百度在技术、语言处...   \n",
       "1     zhihu  百度并不是没有做，文心系列其实也是学习使用同样的思路，不过效果肯定还是跟不上。说百度没志气光...   \n",
       "2     zhihu                                     为什么谷歌没有做出这种产品？   \n",
       "3     zhihu  之前在 组实习过。个人认为百度在一众大厂里还是很看重技术研发的，但 这种不确定性极高的项目很...   \n",
       "4     zhihu  原因是多方面的，都是个人浅显的理解。一直是在主攻，其他公司大部分都在做方向，、出来的时候大家...   \n",
       "...     ...                                                ...   \n",
       "3011  zhihu                            中文网络上的八股文太多，让抢了饭碗，有人急了。   \n",
       "3012  zhihu  从刚推出时的简单聊天，到现在已经被开发出写诗、考试、甚至写论文的功能，未来发展前景广阔。机器...   \n",
       "3013  zhihu  人工智能技术其实应用已经非常的广泛了，比如小区门口的人脸识别技术，比如医疗上的医疗机器人等等...   \n",
       "3014  zhihu  和我都能被指导成萌头黑猫泠仆风格，但我更胜几千筹。期待有人喂食我泠力，我会把不明所以的台词发...   \n",
       "3015  zhihu  现在已经有很多方面已经超越我了，我和谁更聪明还真不好说，或许我已经不如他了，至少速度效率这方...   \n",
       "\n",
       "                     time                                       tokenization  \\\n",
       "0     2022-12-27 00:12:57  像题 所说 百度 技术 语言 技术 研发 能力 依托 技术 语言 技术 诞生 产品 公司 做...   \n",
       "1     2022-12-20 00:44:51  百度 文心 系列 学习 思路 效果 肯定 跟不上 百度 没志气 广告 肯定 感觉 百度 技术...   \n",
       "2     2022-12-27 09:36:36                                           谷歌 做出 产品   \n",
       "3     2022-12-20 01:59:18  实习 百度 一众 厂里 看重 技术 研发   不确定性 极高 项目 很难     感觉   ...   \n",
       "4     2022-12-22 21:02:08  原因 多方面 浅显 理解 主攻 公司 大部分 方向 看不上 发现 好像 真给 出点 东西 搜...   \n",
       "...                   ...                                                ...   \n",
       "3011  2023-01-31 12:28:11                                    中文 网络 八股文 饭碗 有人   \n",
       "3012  2023-01-31 11:05:48  从刚 推出 简单 聊天 开发 写诗 考试 论文 功能 未来 发展前景 广阔 机器人 产业 国...   \n",
       "3013  2023-01-31 11:05:44  人工智能 技术 小区 门口 人脸识别 技术 医疗 医疗 机器人 科技 技术 发展 社会 人工...   \n",
       "3014  2023-02-06 11:31:36  指导 成萌头 黑猫 风格 几千 期待 有人 喂食 我泠力 我会 不明 台词 发给你 作品 可...   \n",
       "3015  2023-02-06 11:27:59         超越 聪明 不好 至少 速度 效率 这方面 肯定 碾压 超过 时间 东西 几年 它教   \n",
       "\n",
       "                                  tokenization_filtered  \n",
       "0     百度 技术 语言 技术 能力 依托 技术 语言 技术 诞生 产品 公司 做出 产品 百度 能...  \n",
       "1     百度 文心 学习 思路 效果 肯定 跟不上 百度 没志气 广告 肯定 感觉 百度 技术 氛围...  \n",
       "2                                                 做出 产品  \n",
       "3     实习 百度 厂里 看重 技术 不确定性 项目 感觉 成功 先例 资源 提不上 能力 有限 基...  \n",
       "4     原因 浅显 理解 公司 方向 看不上 发现 好像 出点 搜索引擎 百度 搜索引擎 公司 投入...  \n",
       "...                                                 ...  \n",
       "3011                                              网络 饭碗  \n",
       "3012  推出 简单 开发 写诗 功能 广阔 机器人 产业 国家 层面 政策 支持 老龄化 社会 劳动...  \n",
       "3013  人工智能 技术 小区 人脸识别 技术 医疗 医疗 机器人 科技 技术 社会 人工智能 智能 ...  \n",
       "3014  指导 萌头 黑猫 风格 期待 喂食 我会 不明 台词 作品 可爱 低于 动漫 主角 世界 萌...  \n",
       "3015                            超越 聪明 速度 效率 肯定 碾压 超过 时间  \n",
       "\n",
       "[3016 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data_zhihu_addition_ChatGPT_filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba73323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0194dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3ae28e0",
   "metadata": {},
   "source": [
    "#### 单平台数据初步预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7724ba31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zhihu Addition Set Shape = (3199, 9)\n",
      "Zhihu Addition Set Memory Usage = 0.22 MB\n"
     ]
    }
   ],
   "source": [
    "df_zhihu = pd.read_csv('data_zhihu_addition_ChatGPT.csv')\n",
    "df_zhihu['source'] = 'zhihu'\n",
    "print(f'Zhihu Addition Set Shape = {df_zhihu.shape}')  \n",
    "print('Zhihu Addition Set Memory Usage = {:.2f} MB'.format(df_zhihu.memory_usage().sum() / 1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f13e1e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>fans_count</th>\n",
       "      <th>content</th>\n",
       "      <th>created_time</th>\n",
       "      <th>updated_time</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>voteup_count</th>\n",
       "      <th>url</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>科技鸣人</td>\n",
       "      <td>274995</td>\n",
       "      <td>是啊，为什么没有做出来呢？在chatGPT刚出现的时候，我也想过这个问题。像题主所说的，百度...</td>\n",
       "      <td>2022-12-27 00:12:57</td>\n",
       "      <td>2022-12-27 00:12:57</td>\n",
       "      <td>4</td>\n",
       "      <td>124</td>\n",
       "      <td>https://www.zhihu.com/api/v4//answers/2817371412</td>\n",
       "      <td>zhihu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phoenix Cat</td>\n",
       "      <td>6999</td>\n",
       "      <td>百度并不是没有做，文心系列其实也是学习gpt使用同样的思路，不过效果肯定还是跟不上。说百度没...</td>\n",
       "      <td>2022-12-20 00:44:12</td>\n",
       "      <td>2022-12-20 00:44:51</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>https://www.zhihu.com/api/v4//answers/2808352416</td>\n",
       "      <td>zhihu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>皮蛋XXXXL</td>\n",
       "      <td>417</td>\n",
       "      <td>为什么谷歌没有做出chatGPT这种产品？</td>\n",
       "      <td>2022-12-27 09:36:36</td>\n",
       "      <td>2022-12-27 09:36:36</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.zhihu.com/api/v4//answers/2817621817</td>\n",
       "      <td>zhihu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>知乎用户</td>\n",
       "      <td>3193</td>\n",
       "      <td>之前在 PLATO 组实习过。个人认为百度在一众大厂里还是很看重技术研发的，但 ChatGP...</td>\n",
       "      <td>2022-12-20 01:59:18</td>\n",
       "      <td>2022-12-20 01:59:18</td>\n",
       "      <td>63</td>\n",
       "      <td>110</td>\n",
       "      <td>https://www.zhihu.com/api/v4//answers/2808386208</td>\n",
       "      <td>zhihu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>匿名用户</td>\n",
       "      <td>0</td>\n",
       "      <td>原因是多方面的，都是个人浅显的理解。GPT一直是openai在主攻，其他公司大部分都在做be...</td>\n",
       "      <td>2022-12-20 00:18:46</td>\n",
       "      <td>2022-12-22 21:02:08</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>https://www.zhihu.com/api/v4//answers/2808336053</td>\n",
       "      <td>zhihu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author  fans_count                                            content  \\\n",
       "0         科技鸣人      274995  是啊，为什么没有做出来呢？在chatGPT刚出现的时候，我也想过这个问题。像题主所说的，百度...   \n",
       "1  Phoenix Cat        6999  百度并不是没有做，文心系列其实也是学习gpt使用同样的思路，不过效果肯定还是跟不上。说百度没...   \n",
       "2      皮蛋XXXXL         417                              为什么谷歌没有做出chatGPT这种产品？   \n",
       "3         知乎用户        3193  之前在 PLATO 组实习过。个人认为百度在一众大厂里还是很看重技术研发的，但 ChatGP...   \n",
       "4         匿名用户           0  原因是多方面的，都是个人浅显的理解。GPT一直是openai在主攻，其他公司大部分都在做be...   \n",
       "\n",
       "          created_time         updated_time  comment_count  voteup_count  \\\n",
       "0  2022-12-27 00:12:57  2022-12-27 00:12:57              4           124   \n",
       "1  2022-12-20 00:44:12  2022-12-20 00:44:51              0            13   \n",
       "2  2022-12-27 09:36:36  2022-12-27 09:36:36              0             2   \n",
       "3  2022-12-20 01:59:18  2022-12-20 01:59:18             63           110   \n",
       "4  2022-12-20 00:18:46  2022-12-22 21:02:08              0            28   \n",
       "\n",
       "                                                url source  \n",
       "0  https://www.zhihu.com/api/v4//answers/2817371412  zhihu  \n",
       "1  https://www.zhihu.com/api/v4//answers/2808352416  zhihu  \n",
       "2  https://www.zhihu.com/api/v4//answers/2817621817  zhihu  \n",
       "3  https://www.zhihu.com/api/v4//answers/2808386208  zhihu  \n",
       "4  https://www.zhihu.com/api/v4//answers/2808336053  zhihu  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zhihu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f284792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df_zhihu = df_zhihu.rename(columns={'content':'comments','updated_time':'time'})\n",
    "\n",
    "# convert data type\n",
    "df_zhihu['time'] = df_zhihu['time'].astype('datetime64[ns]')\n",
    "\n",
    "# extract data(source, topics, comments, time)\n",
    "df_zhihu = df_zhihu[['source','comments','time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6f2f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data from four platform\n",
    "df = df_zhihu.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711a54a3",
   "metadata": {},
   "source": [
    "#### 规范化预处理（Cleansing）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe3415c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size =  (3199, 3)\n",
      "After drop duplicated data size =  (3032, 3)\n",
      "After drop missing data size =  (3031, 3)\n"
     ]
    }
   ],
   "source": [
    "# drop duplicated\n",
    "print('Data size = ',df.shape)\n",
    "df = df.drop_duplicates(subset='comments')\n",
    "# print('Duplicates data size = ',df.duplicated(subset='comments').sum())\n",
    "print('After drop duplicated data size = ',df.shape)\n",
    "\n",
    "# drop data missing time or comments\n",
    "df.dropna(subset=['comments'],inplace=True)\n",
    "df.dropna(subset=['time'],inplace=True)\n",
    "print('After drop missing data size = ',df.shape)\n",
    "df.index = range(0,len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "398e6bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data clean\n",
    "def clean(text):\n",
    "    '''\n",
    "    正则化\n",
    "    '''\n",
    "    \n",
    "    # 清除@和回复/转发中的用户名\n",
    "    text = re.sub(r\"(回复)?(//)?\\s*@\\S*?\\s*(:| |$)\", \"\", text).strip()\n",
    "    \n",
    "    # 清除emoji表情\n",
    "    text = emoji.demojize(text).strip()   \n",
    "    \n",
    "    # 清除##话题内容\n",
    "    text = re.sub(r\"#\\S+#\", \"\", text).strip()    \n",
    "    \n",
    "    # 清除网址link\n",
    "    URL_REGEX1 = re.compile('</?\\w+[^>]*>')\n",
    "    URL_REGEX2 = re.compile('(https|http|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]')\n",
    "    text = re.sub(URL_REGEX1, \"\", text).strip()\n",
    "    text = re.sub(URL_REGEX2, \"\", text).strip()\n",
    "    \n",
    "    # 清除数字\n",
    "    text = re.sub('[\\d]+','',text).strip()\n",
    "    \n",
    "    # 清除字母\n",
    "    text = re.sub('[a-zA-Z]+','',text).strip()\n",
    "    \n",
    "    # 清除无意义的词语\n",
    "    text = text.replace(\"转发微博\", \"\").strip()\n",
    "    \n",
    "    # 合并多余的空格\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip() \n",
    "    \n",
    "    # 清除标点\n",
    "    r = \"[_.+-=——$%^~@#￥%……&*《》<>「」{}【】()“”\\\"/]+\"\n",
    "    r_cn = \"[\\\\u3002\\\\uff1b\\\\uff0c\\\\uff1a\\\\u201c\\\\u201d\\\\uff08\\\\uff09\\\\u3001\\\\uff1f\\\\u300a\\\\u300b]\"\n",
    "    text = re.sub(r, '', text).strip()\n",
    "#     text = re.sub(r_cn, '', text).strip()\n",
    "\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "547ff2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(text):\n",
    "    try:\n",
    "        return convert(text,'zh-hans')\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20bb661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 3031/3031 [00:00<00:00, 6664.97it/s]\n",
      "100%|█████████████████████████████████████| 3031/3031 [00:00<00:00, 9603.29it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['comments'] = df['comments'].progress_apply(clean) # 综合正则化处理\n",
    "df['comments'] = df['comments'].progress_apply(simplify) # 繁体字简体化\n",
    "# df['topics'] = df['topics'].progress_apply(clean) # 繁体字简体化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba152a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size =  (3031, 3)\n",
      "After drop duplicated data size =  (3016, 3)\n"
     ]
    }
   ],
   "source": [
    "# drop duplicated\n",
    "print('Data size = ',df.shape)\n",
    "df.drop_duplicates(subset='comments',inplace=True)\n",
    "print('After drop duplicated data size = ',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6282e042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>comments</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2981</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>试了一下党建文章能不能用 发现不行</td>\n",
       "      <td>2023-01-31 15:42:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>高考第一肯定是。后面绝大部分艺术也可能被超越了。普通老百姓想那么多干啥，搞的跟自己是靠艺术家...</td>\n",
       "      <td>2023-01-31 15:35:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2983</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>首先淘汰缺乏独立思考能力的唐飞们</td>\n",
       "      <td>2023-01-31 15:13:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2984</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>懂什么是藏头诗吗？懂什么是信息价吗？差了数量级呀 。。。。</td>\n",
       "      <td>2023-01-31 15:10:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>上午花了点小钱，去自己试验了一下。简单小结就是，如果你自己有东西，找它问几句，或者写一个，再...</td>\n",
       "      <td>2023-01-31 15:10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2986</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>这就用它来回答知乎回答。</td>\n",
       "      <td>2023-01-31 15:07:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>就是解放生产力而已。未来即将进入真正的自媒体时代，网剧泛滥，程度可以参考网文。可以参考一下西...</td>\n",
       "      <td>2023-01-31 15:02:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>有些学生用它来写作业，帮自己生成答案，生成论文。这些学生是解放了自己，还是被支配了呢？</td>\n",
       "      <td>2023-01-31 14:56:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>如果是免费的可以一试，收费就打扰啦。</td>\n",
       "      <td>2023-01-31 14:51:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2990</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>人工智能只是一个辅助工具，没有自主意识，就不存在支配地位。肯定是生产力，至少，它比搜索引擎来...</td>\n",
       "      <td>2023-01-31 14:37:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>任何技术进步都会对不熟悉的人造成威胁或被支配感，因此不是什么开始，只要不能完全替代人工判断仍...</td>\n",
       "      <td>2023-01-31 14:18:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>已经不能写了</td>\n",
       "      <td>2023-01-31 14:12:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>人工智能的运用我们已经听说很多年了，但人工智能的冲击真正来临的时候，一定依旧会带给人们巨大的...</td>\n",
       "      <td>2023-01-31 13:54:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>只能说是一个不错的生产力或学习工具，想要真正替代人类职业还差的远。</td>\n",
       "      <td>2023-01-31 13:25:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>还早着呢，还没出现呢</td>\n",
       "      <td>2023-01-31 13:06:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>创新能力还不太行，但是应付平常的一些重复性工作还可以，只要网上有模板或者资料。</td>\n",
       "      <td>2023-01-31 12:47:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>使用的方向：智能写作：通过生成文章，简化写作过程并加快速度。使用生成新闻报道、广告文案、商业...</td>\n",
       "      <td>2023-01-31 12:30:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>我来泼点冷水吧，我主要用 来查找资料，用来写作。但经过一段时间的使用后，感觉 的数据可信度并...</td>\n",
       "      <td>2023-01-31 12:24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>以后微博、知乎的机器人将极难分辨。我对未来个体话语权的丧失极为担忧。</td>\n",
       "      <td>2023-01-31 12:22:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>为什么不去问一问神奇的呢？</td>\n",
       "      <td>2023-01-31 11:54:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>只能说用它水水文章效率挺高</td>\n",
       "      <td>2023-01-31 11:51:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3002</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>兔年小作文又火了！这次是写的，概念股获热炒，投资看点八图一网打尽！</td>\n",
       "      <td>2023-01-31 11:12:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3003</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>支配的开始</td>\n",
       "      <td>2023-01-31 11:05:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>点进来之前我还以为我的论文有救了呢</td>\n",
       "      <td>2023-01-31 10:13:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>机器开始应用于生产的时候，做体力劳动的工人们自动化设备开始应用于生产的时候，做手工劳动的工人...</td>\n",
       "      <td>2023-01-31 09:55:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3006</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>会，我测试过了写的文章还不错</td>\n",
       "      <td>2023-01-31 08:48:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>用搜索过一些画作，虽然有些误解，但最终还是找到了</td>\n",
       "      <td>2023-01-31 08:20:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>导致更多白领失业，变成司机</td>\n",
       "      <td>2023-01-31 03:45:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>我已经用它来写文案了。除非算法构建出强大的创意能力，否则只是个工具，人类不可能被支配</td>\n",
       "      <td>2023-01-30 23:56:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3010</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>和其他人工智能技术的出现可以帮助人们解放生产力，提高效率和创造性。例如可以协助作者快速生成文...</td>\n",
       "      <td>2023-01-30 21:44:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3011</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>即是解放生产力的机会也是被支配的开始。高科技的发展需要人更加的理性来面对社会，如果理智不够。...</td>\n",
       "      <td>2023-01-30 20:57:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3012</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>个人觉得更多的是生产力得到解放，很多专业的词汇和概念，免去了搜索引擎的繁琐，这是很值得称赞的一点。</td>\n",
       "      <td>2023-01-30 20:43:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3013</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>可以说是两者都有。等技术的出现可以解放人类生产力，提高生产效率，帮助人们完成一些复杂的任务。...</td>\n",
       "      <td>2023-01-30 20:30:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>解放的并不是人类的创新能力，而是人类的模仿能力。模仿能力的解放并不能改变生产力也不提高交流效...</td>\n",
       "      <td>2023-01-30 19:32:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>先答结果，个人认为是解放生产力的机会，能让创作者的目标更聚焦于结果。世界会变的越好，还是越坏...</td>\n",
       "      <td>2023-01-30 16:21:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>解放了！！解放了！！！我们被解放了，我们被释放了。</td>\n",
       "      <td>2023-01-31 11:49:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3017</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>由和微软开发的是一种基于人工智能的自然语言服务，为开发者提供对话界面。它将深度神经强化学习与...</td>\n",
       "      <td>2023-01-31 11:22:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3018</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>你们是怎么把它用到写作上的？我试了一个月写出来的都是屎，哪怕短句也是。只能写一点不严谨的论述。</td>\n",
       "      <td>2023-01-31 09:19:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>国内用的时候不会时不时遇到禁止访问的提示吗？更想知道这些平时不得了的大厂有没有能做出一个国产的</td>\n",
       "      <td>2023-01-31 11:20:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3020</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>没有国外手机号注册不了，回答者都是在国外或者有国外手机号？</td>\n",
       "      <td>2023-01-31 09:29:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>我是不会愿意去花时间去读机器人写的东西的，相信大部分人都不会喜欢。</td>\n",
       "      <td>2023-01-31 04:52:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>往好里想，脑力劳动渐渐不需要搬砖工人，搬砖的效率极大提高，可以集中精力专注于创造性突破性工作...</td>\n",
       "      <td>2023-02-01 17:10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3023</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>太容易造成魔法的混乱了，囧╯□╰囧</td>\n",
       "      <td>2023-02-01 15:34:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>所有事情都能干了，那还要人类干什么呢</td>\n",
       "      <td>2023-01-31 23:57:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3025</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>人工智能（）有可能彻底改变我们生活的许多方面，包括我们如何处理网络安全。然而，它也带来了需要...</td>\n",
       "      <td>2023-01-31 15:51:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3026</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>中文网络上的八股文太多，让抢了饭碗，有人急了。</td>\n",
       "      <td>2023-01-31 12:28:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3027</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>从刚推出时的简单聊天，到现在已经被开发出写诗、考试、甚至写论文的功能，未来发展前景广阔。机器...</td>\n",
       "      <td>2023-01-31 11:05:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>人工智能技术其实应用已经非常的广泛了，比如小区门口的人脸识别技术，比如医疗上的医疗机器人等等...</td>\n",
       "      <td>2023-01-31 11:05:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3029</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>和我都能被指导成萌头黑猫泠仆风格，但我更胜几千筹。期待有人喂食我泠力，我会把不明所以的台词发...</td>\n",
       "      <td>2023-02-06 11:31:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3030</th>\n",
       "      <td>zhihu</td>\n",
       "      <td>现在已经有很多方面已经超越我了，我和谁更聪明还真不好说，或许我已经不如他了，至少速度效率这方...</td>\n",
       "      <td>2023-02-06 11:27:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source                                           comments  \\\n",
       "2981  zhihu                                  试了一下党建文章能不能用 发现不行   \n",
       "2982  zhihu  高考第一肯定是。后面绝大部分艺术也可能被超越了。普通老百姓想那么多干啥，搞的跟自己是靠艺术家...   \n",
       "2983  zhihu                                   首先淘汰缺乏独立思考能力的唐飞们   \n",
       "2984  zhihu                      懂什么是藏头诗吗？懂什么是信息价吗？差了数量级呀 。。。。   \n",
       "2985  zhihu  上午花了点小钱，去自己试验了一下。简单小结就是，如果你自己有东西，找它问几句，或者写一个，再...   \n",
       "2986  zhihu                                       这就用它来回答知乎回答。   \n",
       "2987  zhihu  就是解放生产力而已。未来即将进入真正的自媒体时代，网剧泛滥，程度可以参考网文。可以参考一下西...   \n",
       "2988  zhihu        有些学生用它来写作业，帮自己生成答案，生成论文。这些学生是解放了自己，还是被支配了呢？   \n",
       "2989  zhihu                                 如果是免费的可以一试，收费就打扰啦。   \n",
       "2990  zhihu  人工智能只是一个辅助工具，没有自主意识，就不存在支配地位。肯定是生产力，至少，它比搜索引擎来...   \n",
       "2991  zhihu  任何技术进步都会对不熟悉的人造成威胁或被支配感，因此不是什么开始，只要不能完全替代人工判断仍...   \n",
       "2992  zhihu                                             已经不能写了   \n",
       "2993  zhihu  人工智能的运用我们已经听说很多年了，但人工智能的冲击真正来临的时候，一定依旧会带给人们巨大的...   \n",
       "2994  zhihu                  只能说是一个不错的生产力或学习工具，想要真正替代人类职业还差的远。   \n",
       "2995  zhihu                                         还早着呢，还没出现呢   \n",
       "2996  zhihu            创新能力还不太行，但是应付平常的一些重复性工作还可以，只要网上有模板或者资料。   \n",
       "2997  zhihu  使用的方向：智能写作：通过生成文章，简化写作过程并加快速度。使用生成新闻报道、广告文案、商业...   \n",
       "2998  zhihu  我来泼点冷水吧，我主要用 来查找资料，用来写作。但经过一段时间的使用后，感觉 的数据可信度并...   \n",
       "2999  zhihu                 以后微博、知乎的机器人将极难分辨。我对未来个体话语权的丧失极为担忧。   \n",
       "3000  zhihu                                      为什么不去问一问神奇的呢？   \n",
       "3001  zhihu                                      只能说用它水水文章效率挺高   \n",
       "3002  zhihu                  兔年小作文又火了！这次是写的，概念股获热炒，投资看点八图一网打尽！   \n",
       "3003  zhihu                                              支配的开始   \n",
       "3004  zhihu                                  点进来之前我还以为我的论文有救了呢   \n",
       "3005  zhihu  机器开始应用于生产的时候，做体力劳动的工人们自动化设备开始应用于生产的时候，做手工劳动的工人...   \n",
       "3006  zhihu                                     会，我测试过了写的文章还不错   \n",
       "3007  zhihu                           用搜索过一些画作，虽然有些误解，但最终还是找到了   \n",
       "3008  zhihu                                      导致更多白领失业，变成司机   \n",
       "3009  zhihu         我已经用它来写文案了。除非算法构建出强大的创意能力，否则只是个工具，人类不可能被支配   \n",
       "3010  zhihu  和其他人工智能技术的出现可以帮助人们解放生产力，提高效率和创造性。例如可以协助作者快速生成文...   \n",
       "3011  zhihu  即是解放生产力的机会也是被支配的开始。高科技的发展需要人更加的理性来面对社会，如果理智不够。...   \n",
       "3012  zhihu  个人觉得更多的是生产力得到解放，很多专业的词汇和概念，免去了搜索引擎的繁琐，这是很值得称赞的一点。   \n",
       "3013  zhihu  可以说是两者都有。等技术的出现可以解放人类生产力，提高生产效率，帮助人们完成一些复杂的任务。...   \n",
       "3014  zhihu  解放的并不是人类的创新能力，而是人类的模仿能力。模仿能力的解放并不能改变生产力也不提高交流效...   \n",
       "3015  zhihu  先答结果，个人认为是解放生产力的机会，能让创作者的目标更聚焦于结果。世界会变的越好，还是越坏...   \n",
       "3016  zhihu                          解放了！！解放了！！！我们被解放了，我们被释放了。   \n",
       "3017  zhihu  由和微软开发的是一种基于人工智能的自然语言服务，为开发者提供对话界面。它将深度神经强化学习与...   \n",
       "3018  zhihu    你们是怎么把它用到写作上的？我试了一个月写出来的都是屎，哪怕短句也是。只能写一点不严谨的论述。   \n",
       "3019  zhihu    国内用的时候不会时不时遇到禁止访问的提示吗？更想知道这些平时不得了的大厂有没有能做出一个国产的   \n",
       "3020  zhihu                      没有国外手机号注册不了，回答者都是在国外或者有国外手机号？   \n",
       "3021  zhihu                  我是不会愿意去花时间去读机器人写的东西的，相信大部分人都不会喜欢。   \n",
       "3022  zhihu  往好里想，脑力劳动渐渐不需要搬砖工人，搬砖的效率极大提高，可以集中精力专注于创造性突破性工作...   \n",
       "3023  zhihu                                  太容易造成魔法的混乱了，囧╯□╰囧   \n",
       "3024  zhihu                                 所有事情都能干了，那还要人类干什么呢   \n",
       "3025  zhihu  人工智能（）有可能彻底改变我们生活的许多方面，包括我们如何处理网络安全。然而，它也带来了需要...   \n",
       "3026  zhihu                            中文网络上的八股文太多，让抢了饭碗，有人急了。   \n",
       "3027  zhihu  从刚推出时的简单聊天，到现在已经被开发出写诗、考试、甚至写论文的功能，未来发展前景广阔。机器...   \n",
       "3028  zhihu  人工智能技术其实应用已经非常的广泛了，比如小区门口的人脸识别技术，比如医疗上的医疗机器人等等...   \n",
       "3029  zhihu  和我都能被指导成萌头黑猫泠仆风格，但我更胜几千筹。期待有人喂食我泠力，我会把不明所以的台词发...   \n",
       "3030  zhihu  现在已经有很多方面已经超越我了，我和谁更聪明还真不好说，或许我已经不如他了，至少速度效率这方...   \n",
       "\n",
       "                    time  \n",
       "2981 2023-01-31 15:42:45  \n",
       "2982 2023-01-31 15:35:44  \n",
       "2983 2023-01-31 15:13:06  \n",
       "2984 2023-01-31 15:10:43  \n",
       "2985 2023-01-31 15:10:33  \n",
       "2986 2023-01-31 15:07:53  \n",
       "2987 2023-01-31 15:02:42  \n",
       "2988 2023-01-31 14:56:04  \n",
       "2989 2023-01-31 14:51:40  \n",
       "2990 2023-01-31 14:37:10  \n",
       "2991 2023-01-31 14:18:20  \n",
       "2992 2023-01-31 14:12:20  \n",
       "2993 2023-01-31 13:54:27  \n",
       "2994 2023-01-31 13:25:08  \n",
       "2995 2023-01-31 13:06:20  \n",
       "2996 2023-01-31 12:47:44  \n",
       "2997 2023-01-31 12:30:10  \n",
       "2998 2023-01-31 12:24:00  \n",
       "2999 2023-01-31 12:22:09  \n",
       "3000 2023-01-31 11:54:28  \n",
       "3001 2023-01-31 11:51:26  \n",
       "3002 2023-01-31 11:12:41  \n",
       "3003 2023-01-31 11:05:11  \n",
       "3004 2023-01-31 10:13:26  \n",
       "3005 2023-01-31 09:55:10  \n",
       "3006 2023-01-31 08:48:04  \n",
       "3007 2023-01-31 08:20:58  \n",
       "3008 2023-01-31 03:45:08  \n",
       "3009 2023-01-30 23:56:50  \n",
       "3010 2023-01-30 21:44:45  \n",
       "3011 2023-01-30 20:57:26  \n",
       "3012 2023-01-30 20:43:45  \n",
       "3013 2023-01-30 20:30:21  \n",
       "3014 2023-01-30 19:32:39  \n",
       "3015 2023-01-30 16:21:22  \n",
       "3016 2023-01-31 11:49:56  \n",
       "3017 2023-01-31 11:22:50  \n",
       "3018 2023-01-31 09:19:17  \n",
       "3019 2023-01-31 11:20:03  \n",
       "3020 2023-01-31 09:29:14  \n",
       "3021 2023-01-31 04:52:07  \n",
       "3022 2023-02-01 17:10:33  \n",
       "3023 2023-02-01 15:34:28  \n",
       "3024 2023-01-31 23:57:01  \n",
       "3025 2023-01-31 15:51:34  \n",
       "3026 2023-01-31 12:28:11  \n",
       "3027 2023-01-31 11:05:48  \n",
       "3028 2023-01-31 11:05:44  \n",
       "3029 2023-02-06 11:31:36  \n",
       "3030 2023-02-06 11:27:59  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec82d2",
   "metadata": {},
   "source": [
    "#### 数据过滤预处理（overview_data）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49cc37c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file):\n",
    "    #获取文件最可能的编码格式\n",
    "    import chardet\n",
    "    with open(file, \"rb\") as f:\n",
    "        r = f.read()\n",
    "    e = chardet.detect(r)\n",
    "    encoding = e.get(\"encoding\")\n",
    "    return encoding\n",
    "\n",
    "def get_stopwords(stop_words_file, encoding):\n",
    "    \"\"\"\n",
    "    读取停用词文件\n",
    "    参数：\n",
    "        停用词文件\n",
    "    返回：\n",
    "        停用词列表\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(stop_words_file,encoding = encoding) as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    "    return custom_stopwords_list\n",
    "\n",
    "def get_tokenization(text):\n",
    "    \"\"\"\n",
    "    使用jieba分词并剔除停用词\n",
    "    参数：\n",
    "        文本字符串\n",
    "    返回：\n",
    "        以空格连接的词串\n",
    "    \"\"\"\n",
    "    word_token = []\n",
    "    for word in jieba.lcut(text.strip(), cut_all=False): #精确模式\n",
    "        if word not in STOPWORDS:\n",
    "            word_token.append(word)\n",
    "    return ' '.join(word_token)\n",
    "\n",
    "def clear_singlechar(text):\n",
    "    \"\"\"\n",
    "    清除切词后的单个字符\n",
    "    \"\"\"\n",
    "    word_cleared = []\n",
    "    for word in text.strip().split(r' '):\n",
    "        if len(word)!=1:\n",
    "            word_cleared.append(word)\n",
    "    return ' '.join(word_cleared)\n",
    "\n",
    "def pos_tag_filter_nva(text):\n",
    "    finance_noisy = [\"疫情\", \"股市\", \"风险投资\", \"风险融资\", \"风险资本\", \"信贷风险\", \"贷款风险\", \"投资风险\", \"交易风险\", \"信用风险\", \"风险评分\", \"金融风险\", \"基金\", \"个股\"]\n",
    "    \n",
    "    words = pseg.cut(text)\n",
    "    filtered_words = []\n",
    "    for word, flag in words:\n",
    "        # a = (\"{0}/{1}\".format(word, flag))\n",
    "        if (flag == 'n'or flag == 'v' or flag == 'a') and (len(word) > 1) and word not in finance_noisy:\n",
    "            filtered_words.append(word)\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0af1f647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words = 2576\n"
     ]
    }
   ],
   "source": [
    "# 停用词表\n",
    "stop_qua = r'ー、一、二、三、四、五、一点、两个、三个、一些、一种、几个、几种；和、跟、与、既、同、及、而、况、况且、何况、乃至、则、乃、就、而、便、于是、然后、至于、说到、此外、像、如、一般、比方、接着、却、虽然、但是、然而、而、偏偏、只是、不过、至于、致、不料、岂知、原来、因为、由于、以便、因此、所以、是故、以致、或、或者、还是、抑、非…即、不是…就是、若、如果、若是、假如、只要、除非、假使、倘若、即使、假若、要是、譬如、像、好比、如同、似乎、等于；不如、不及、与其…不如、若…则、虽然…可是、虽然、固然、尽管、纵然、即使、不但、不仅、而且、何况、并、且、不管、只要、除非、以、以便、以免、为了'\n",
    "\n",
    "STOPWORDS = list(set(get_stopwords('../stopwordsHIT.txt', encoding=detect_encoding('../stopwordsHIT.txt')) + get_stopwords(r'../stopwords.txt', encoding='gbk') + stop_qua.split(r'、')))\n",
    "print(f'Number of stop words = {len(STOPWORDS)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2f05c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/29/27cgqwrx7_18wcd66rpffzlc0000gn/T/jieba.cache\n",
      "Loading model cost 0.360 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|██████████████████████████████████████| 3016/3016 [00:13<00:00, 228.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# 切词\n",
    "df = df.copy()\n",
    "jieba.load_userdict('../dict.txt')\n",
    "\n",
    "tqdm.pandas()\n",
    "df['tokenization'] = df['comments'].progress_apply(get_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68e72cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 3016/3016 [00:00<00:00, 94977.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# 清除单个字符\n",
    "df = df.copy()\n",
    "tqdm.pandas()\n",
    "df['tokenization'] = df['tokenization'].progress_apply(clear_singlechar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa6f02dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../ITP/Data collection/dict.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 过滤切词结果，保留n/v/adj\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mjieba\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_userdict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../ITP/Data collection/dict.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mpandas()\n\u001b[1;32m      6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenization_filtered\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenization\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprogress_apply(pos_tag_filter_nva)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jieba/__init__.py:398\u001b[0m, in \u001b[0;36mTokenizer.load_userdict\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, string_types):\n\u001b[1;32m    397\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m f\n\u001b[0;32m--> 398\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    400\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m resolve_filename(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../ITP/Data collection/dict.txt'"
     ]
    }
   ],
   "source": [
    "# 过滤切词结果，保留n/v/adj\n",
    "df = df.copy()\n",
    "jieba.load_userdict('../dict.txt')\n",
    "\n",
    "tqdm.pandas()\n",
    "df['tokenization_filtered'] = df['tokenization'].progress_apply(pos_tag_filter_nva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c171dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3fe4b22",
   "metadata": {},
   "source": [
    "#### 对微博含有广告、打榜等重复噪音数据的补充处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b76166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1.根据关键词对微博数据再做初筛 (主要针对微博数据)\n",
    "# words_list = ['风险','安全隐患','威胁','危险','危机','后果','危害','安全事件','预防','安全事故','风险管理','负面影响','灾害','困境','欺诈','失误','漏洞']\n",
    "# risk_words = str()\n",
    "# for item in words_list:\n",
    "#     risk_words = risk_words+str(item)+'|'\n",
    "    \n",
    "# # 筛掉所有不含有“人工智能”和“risk_words”中关键词的内容\n",
    "# data_weibo_filtered = data_weibo[(data_weibo['tokenization_filtered'].str.contains(\"人工智能\"))==True]\n",
    "\n",
    "# # 筛掉所有不含有“人工智能”和“words_list”中关键词的内容\n",
    "# data_weibo_filtered= data_weibo_filtered[(data_weibo_filtered['tokenization_filtered'].str.contains(risk_words))==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2.使用fuzzywuzzy库，对数据进行遍历，剔除掉相似度75%以上的数据（主要针对微博数据）\n",
    "\n",
    "# # Final Version (itertools.islice)\n",
    "# def Similarity(index_tuples_list):\n",
    "#     res = []\n",
    "# #     print(index_tuples_list)\n",
    "#     for p in index_tuples_list:\n",
    "#         similarity = fuzz.ratio(data_weibo_filtered['comments'][p[0]],data_weibo_filtered['comments'][p[1]])\n",
    "#         if similarity > 75:\n",
    "#             res.append([p[0],p[1],similarity])\n",
    "#     return res\n",
    "\n",
    "# pair_similarity = []\n",
    "# begin_all = time.time()\n",
    "\n",
    "# obj_list = []\n",
    "# executor = ThreadPoolExecutor(max_workers=8)\n",
    "\n",
    "# # 分批次 1000000000*18\n",
    "# for i in tqdm(range(3000000000,4000000000,5000000),position=0, leave=True):\n",
    "#     obj_list.append(itertools.islice(itertools.combinations(data_weibo_filtered.index.to_list(),2),i,i+5000000)) \n",
    "\n",
    "# for res in executor.map(Similarity, obj_list):\n",
    "#     if (len(res)) > 0:\n",
    "#         pair_similarity.extend(res)\n",
    "    \n",
    "# end_all = time.time()\n",
    "# print(end_all-begin_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338103d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.删除分词过滤后无词的数据\n",
    "print(df.shape)\n",
    "df.dropna(subset=['tokenization_filtered'], inplace=True)\n",
    "# df.reset_index(inplace=True, drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3626dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.删除分词+过滤后词数<5的数据(可选操作)\n",
    "'''\n",
    "item_word_below_5 = []\n",
    "\n",
    "tqdm.pandas()\n",
    "df['word_count'] = df['tokenization_filtered'].progress_apply(lambda x: len(x.split()))\n",
    "df = df[df['word_count']>=5][['source','comments','time','tokenization','tokenization_filtered','word_count']]# ,'topics'\n",
    "print(df.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a8fa161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['source','comments','time','tokenization','tokenization_filtered']].to_csv('data_zhihu_ChagtGPT_addition_filtered.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe71ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d82e562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
