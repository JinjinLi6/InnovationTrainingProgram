{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "845551de",
   "metadata": {},
   "source": [
    "## Zhihu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8b9f64",
   "metadata": {},
   "source": [
    "**知乎爬虫主要通过api调用获取数据信息,关键词搜索数据返回api暂时在维修中无法使用，其通过Web Scraper插件使用实现。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0981a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from selectolax.parser import HTMLParser\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2deb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_answer(question_id, start):\n",
    "#    基于问题id获取单个问题下所有回答数据信息\n",
    "    print(f\"问题{question_id},第{start}条。\")\n",
    "    url = \"https://www.zhihu.com/api/v4//questions/{}/answers\".format(question_id)\n",
    "    print(url)\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Mobile Safari/537.36',\n",
    "               'referer':'https://www.zhihu.com/question/{}'.format(question_id),\n",
    "              } \n",
    "    \n",
    "    para = {\n",
    "    'include':'data[*].is_normal,admin_closed_comment, \\\n",
    "    reward_info,is_collapsed,annotation_action,annotation_detail,\\\n",
    "    collapse_reason,is_sticky,collapsed_by,suggest_edit,comment_count,\\\n",
    "    can_comment,content,editable_content,attachment,voteup_count,\\\n",
    "    reshipment_settings,comment_permission,created_time,\\\n",
    "    updated_time,review_info,relevant_info,question,excerpt,is_labeled,\\\n",
    "    paid_info,paid_info_content,relationship.is_authorized,is_author,voting,is_thanked,\\\n",
    "    is_nothelp,is_recognized;data[*].mark_infos[*].url;data[*].author.follower_count,\\\n",
    "    vip_info,badge[*].topics;data[*].settings.table_of_content.enabled',\n",
    "    'offset':start,\n",
    "    'limit':20,\n",
    "    'sort_by':'default',\n",
    "    'platform':'desktop'\n",
    "    }\n",
    "    \n",
    "    global df   \n",
    "\n",
    "    response = requests.get(url,params=para,headers = headers)\n",
    "    response.encoding = response.apparent_encoding\n",
    "    if response.status_code == 200:\n",
    "        res = json.loads(response.text)\n",
    "        if res['data']:\n",
    "            for answer in res['data']:\n",
    "                author = answer['author']['name']\n",
    "                fans = answer['author']['follower_count'] \n",
    "                content = HTMLParser(answer['content']).text()\n",
    "                #content = answer['content']\n",
    "                created_time = datetime.datetime.fromtimestamp(answer['created_time'])\n",
    "                updated_time = datetime.datetime.fromtimestamp(answer['updated_time'])\n",
    "                comment = answer['comment_count']\n",
    "                voteup = answer['voteup_count']\n",
    "                link = answer['url']\n",
    "                row = {\n",
    "                    'author':[author],\n",
    "                    'fans_count':[fans],\n",
    "                    'content':[content],\n",
    "                    'created_time':[created_time],\n",
    "                    'updated_time':[updated_time],\n",
    "                    'comment_count':[comment],\n",
    "                    'voteup_count':[voteup],\n",
    "                    'url':[link]\n",
    "                }\n",
    "                df = df.append(pd.DataFrame(row),ignore_index=True)\n",
    "            if len(res['data'])==20: \n",
    "                download_answer(question_id,start+20)\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a57b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_list = list(pd.read_csv(r\"C:\\Users\\Ljinjin\\Desktop\\data\\zhihu\\zhihu_urls.csv\").iloc[:,4])\n",
    "#Web Scraper插件也可获取问题id\n",
    "\n",
    "# 知乎API维护中...\n",
    "# def download_question(risk_keyword, start):\n",
    "# #     基于风险关键词获取问题id\n",
    "#     url = 'https://www.zhihu.com/api/v4/search_v3?'\n",
    "#     headers = {\n",
    "#         'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Mobile Safari/537.36',\n",
    "#     }\n",
    "#     para = {\n",
    "#         'gk_version':'gz-gaokao',\n",
    "#         't':'general',\n",
    "#         'q':'人工智能 {}'.format(risk_keyword),\n",
    "#         'correction':1,\n",
    "#         'offset':start,\n",
    "#         'limit':20,\n",
    "#         'filter_fields':'',\n",
    "#         'lc_idx':0,\n",
    "#         'show_all_topics':0,\n",
    "#         'search_source':'Normal'\n",
    "#     }    \n",
    "#     response = requests.get(url,params=para,headers = headers)\n",
    "#     response.encoding = response.apparent_encoding\n",
    "#     if response.status_code == 200:\n",
    "#         print(response.text)\n",
    "#         return response.text\n",
    "#     else:\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionId = []\n",
    "def getQuestionId(href):\n",
    "    res = re.findall(r'https://www.zhihu.com/question/([0-9]+)',href)\n",
    "    if(len(res)!=0):\n",
    "        questionId.append(res[0])\n",
    "for url in urls_list:\n",
    "    getQuestionId(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e70fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=('author','fans_count','content','created_time','updated_time','comment_count','voteup_count','url'))\n",
    "\n",
    "for id in questionId:\n",
    "    try:\n",
    "        download_answer(id,0)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c51336",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"./zhihu.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20155f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "681eedbd",
   "metadata": {},
   "source": [
    "## Jianshu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e469f",
   "metadata": {},
   "source": [
    "**基于Selenium+直接页面解析获取数据的爬虫实现。**  \n",
    "此方式集中爬取消耗时间过长，推荐**分批次**爬取数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c782816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "def generate_urls():\n",
    "\n",
    "    list = driver.find_elements_by_css_selector(\"a.title\")\n",
    "    linkset = set()\n",
    "    for item in list:\n",
    "        try :\n",
    "            link = item.get_attribute(\"href\")\n",
    "            linkset.add(link)\n",
    "        except:\n",
    "            continue\n",
    "    print(linkset)\n",
    "    return linkset\n",
    "\n",
    "def generate_comments_one_page(url):\n",
    "        # title author time content url num\n",
    "        import time\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"h1._1RuRku\")))\n",
    "        except:\n",
    "            print(\"access failed\")\n",
    "            return\n",
    "\n",
    "\n",
    "        try:\n",
    "            title = driver.find_element_by_css_selector(\"h1._1RuRku\").text\n",
    "        except:\n",
    "            title = \"\"\n",
    "            print(\"title missing\")\n",
    "\n",
    "        try :\n",
    "            author = driver.find_element_by_css_selector(\".FxYr8x a\").text\n",
    "        except:\n",
    "            author = \"\"\n",
    "            print(\"author missing\")\n",
    "\n",
    "        try:\n",
    "            time = driver.find_element_by_css_selector(\"time\").text\n",
    "        except:\n",
    "            time = \"\"\n",
    "            print(\"time missing\")\n",
    "\n",
    "        try:\n",
    "            content = driver.find_element_by_css_selector(\"article\").text\n",
    "        except:\n",
    "            content = \"\"\n",
    "            print(\"content missing\")\n",
    "\n",
    "        try:\n",
    "            support = driver.find_element_by_css_selector(\"span._1LOh_5\").text\n",
    "        except:\n",
    "            support = \"\"\n",
    "            print(\"support missing\")\n",
    "\n",
    "\n",
    "        comment = [title, author, time, content, url, support]\n",
    "        return comment\n",
    "\n",
    "\n",
    "def write_csv(datalist, path):\n",
    "    headers=[\"title\",\"author\",\"time\",\"content\",\"url\",\"num\"]\n",
    "    test = pd.DataFrame(columns=headers,data=datalist)\n",
    "    test.to_csv(path, encoding='utf-8-sig')\n",
    "    return test\n",
    "\n",
    "def one_time_crawl(url):  # 利用set去重\n",
    "    # 访问搜索页\n",
    "\n",
    "    for i in range(0,101):\n",
    "        cur_page = url.replace(r\"page=1\",\"page=\"+str(i))\n",
    "        driver.get(cur_page)\n",
    "        # 等待页面加载成功\n",
    "        refresh()\n",
    "        # 最近三天 按钮\n",
    "        driver.find_element_by_css_selector(\"svg\").click()\n",
    "        three_day_element = driver.find_element_by_css_selector(\"li.v-select-options-item:nth-of-type(4)\")\n",
    "        ActionChains(driver).move_to_element(three_day_element).click()\n",
    "        # 读取当前页文章url\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"a.title\")))\n",
    "        for item in generate_urls():\n",
    "            urls.add(item)\n",
    "        print(len(urls))\n",
    "\n",
    "        time.sleep(0.5)\n",
    "    write_url()\n",
    "    return\n",
    "\n",
    "\n",
    "def refresh():\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a.title\")))\n",
    "    except:\n",
    "        driver.refresh()\n",
    "        refresh()\n",
    "\n",
    "\n",
    "def write_url():\n",
    "    header2 = [\"urls\"]\n",
    "    test = pd.DataFrame(columns=header2,data=urls)\n",
    "    test.to_csv(r\"./jianshu_urls.csv\", encoding='utf-8-sig')\n",
    "\n",
    "def input_url():\n",
    "    urls={\n",
    "    \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E9%A3%8E%E9%99%A9&page=1&type=note\" #人工智能 风险\n",
    "    \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%A8%81%E8%83%81&page=1&type=note\" #人工智能 威胁\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%AE%89%E5%85%A8%E9%9A%90%E6%82%A3&page=1&type=note\" #人工智能 安全隐患\n",
    "    ,\"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%8D%B1%E9%99%A9&page=1&type=note\" #人工智能 危险\n",
    "    ,\"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%8D%B1%E6%9C%BA&page=1&type=note\" #人工智能 危机\n",
    "    \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%90%8E%E6%9E%9C&page=1&type=note\" #人工智能 后果\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%8D%B1%E5%AE%B3&page=1&type=note\" #人工智能 危害\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%8D%B1%E9%99%A9&page=1&type=note\" #人工智能 安全事件\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E9%A2%84%E9%98%B2&page=1&type=note\" #人工智能 预防\n",
    "    ,\"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%AE%89%E5%85%A8%E4%BA%8B%E6%95%85&page=1&type=note\" #人工智能 安全事故\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%8D%B1%E9%99%A9&page=1&type=note\" #人工智能 风险管理\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E8%B4%9F%E9%9D%A2%E5%BD%B1%E5%93%8D&page=1&type=note\" #人工智能 负面影响\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E7%81%BE%E5%AE%B3&page=1&type=note\" #人工智能 灾害\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%9B%B0%E5%A2%83&page=1&type=note\" #人工智能 困境  \n",
    "    ,\"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E6%AC%BA%E8%AF%88&page=1&type=note\" #人工智能 欺诈\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E6%AC%BA%E8%AF%88&page=1&type=note\" #人工智能 失误\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E6%BC%8F%E6%B4%9E&page=1&type=note\" #人工智能 漏洞\n",
    "    }\n",
    "    return urls\n",
    "#     try:\n",
    "#         data = pd.read_csv(r\"./data/jianshu_urls.csv\",encoding= 'utf-8-sig')\n",
    "#         return set(data[\"urls\"].tolist())\n",
    "\n",
    "#     except:\n",
    "#         print(\"url file not exist\")\n",
    "\n",
    "urls = input_url()\n",
    "driver = webdriver.Chrome(r'C:\\Users\\Ljinjin\\AppData\\Local\\Google\\Chrome\\chromedriver.exe')\n",
    "comments = []\n",
    "urls_to_scrape = [#\"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E9%A3%8E%E9%99%A9&page=1&type=note\" #人工智能 风险\n",
    "     \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%A8%81%E8%83%81&page=1&type=note\" #人工智能 威胁\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%AE%89%E5%85%A8%E9%9A%90%E6%82%A3&page=1&type=note\" #人工智能 安全隐患\n",
    "    ,\"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%8D%B1%E9%99%A9&page=1&type=note\" #人工智能 危险\n",
    "    ,\"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%8D%B1%E6%9C%BA&page=1&type=note\" #人工智能 危机     \n",
    "    ,\"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%90%8E%E6%9E%9C&page=1&type=note\" #人工智能 后果\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%8D%B1%E5%AE%B3&page=1&type=note\" #人工智能 危害\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%8D%B1%E9%99%A9&page=1&type=note\" #人工智能 安全事件\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E9%A2%84%E9%98%B2&page=1&type=note\" #人工智能 预防\n",
    "    ,\"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%AE%89%E5%85%A8%E4%BA%8B%E6%95%85&page=1&type=note\" #人工智能 安全事故\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%8D%B1%E9%99%A9&page=1&type=note\" #人工智能 风险管理\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E8%B4%9F%E9%9D%A2%E5%BD%B1%E5%93%8D&page=1&type=note\" #人工智能 负面影响\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E7%81%BE%E5%AE%B3&page=1&type=note\" #人工智能 灾害\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E5%9B%B0%E5%A2%83&page=1&type=note\" #人工智能 困境  \n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E6%AC%BA%E8%AF%88&page=1&type=note\" #人工智能 欺诈\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E6%AC%BA%E8%AF%88&page=1&type=note\" #人工智能 失误\n",
    "    , \"https://www.jianshu.com/search?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20%E6%BC%8F%E6%B4%9E&page=1&type=note\" #人工智能 漏洞\n",
    "]\n",
    "\n",
    "\n",
    "while (len(str(urls)) < 7500):\n",
    "    for url_to_scrape in urls_to_scrape:\n",
    "        one_time_crawl(url_to_scrape)\n",
    "\n",
    "for url in urls:\n",
    "    print(url)\n",
    "    this_comment = generate_comments_one_page(url)\n",
    "    if this_comment is not None:\n",
    "        comments.append(this_comment)\n",
    "    print(len(comments))\n",
    "    time.sleep(0.3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = r'./jianshu.csv' \n",
    "write_csv(comments, path)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47140c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "386f4f5a",
   "metadata": {},
   "source": [
    "## Douban"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f1375",
   "metadata": {},
   "source": [
    "**基于Selenium+直接页面解析获取数据的爬虫实现。**  \n",
    "此方式集中爬取消耗时间过长，推荐**分批次**爬取数据；此外模拟浏览器的豆瓣首页弹出时可通过自身**账号登录**以保证后续爬取数据的顺利。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time, datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "driver = webdriver.Chrome(r'C:\\Users\\Ljinjin\\AppData\\Local\\Google\\Chrome\\chromedriver.exe')  #模拟器存放文件夹的位置\n",
    "driver.get('https://www.douban.com/')\n",
    "time.sleep(20)\n",
    "\n",
    "\n",
    "def generate_urls():\n",
    "    \"\"\"\n",
    "    获取人工智能搜索页的豆瓣日记信息\n",
    "    \"\"\"\n",
    "    base_urls = [\n",
    "           \"https://www.douban.com/search?cat=1015&q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD\"  #人工智能, \n",
    "        ,\"https://www.douban.com/search?cat=1015&q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD+%E9%A3%8E%E9%99%A9\"  #人工智能 风险\n",
    "        ,\"https://www.douban.com/search?cat=1015&q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD+%E5%A8%81%E8%83%81\" #人工智能+威胁\n",
    "        ,\"https://www.douban.com/search?cat=1015&q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD+%E5%AE%89%E5%85%A8%E9%9A%90%E6%82%A3\" #安全隐患\n",
    "        ,\"https://www.douban.com/search?cat=1015&q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD+%E5%8D%B1%E9%99%A9\" #危险\n",
    "        ,\"https://www.douban.com/search?cat=1015&q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD+%E5%8D%B1%E6%9C%BA\" #危机\n",
    "        ,\"https://www.douban.com/search?cat=1015&q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD+%E5%90%8E%E6%9E%9C\" #后果 2\n",
    "        ,\"https://www.douban.com/search?cat=1015&q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD+%E5%8D%B1%E5%AE%B3\" #危害 2\n",
    "        ,\"https://www.douban.com/search?cat=1015&q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD+%E5%8D%B1%E9%99%A9\" #危险 2\n",
    "        ,\"https://www.douban.com/search?cat=1015&q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD+%E9%A2%84%E9%98%B2\" #预防 3\n",
    "        ,\"https://www.douban.com/search?cat=1015&q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD+%E5%AE%89%E5%85%A8%E4%BA%8B%E6%95%85\" #安全事故 3\n",
    "        ,\"https://www.douban.com/search?cat=1015&q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD+%E8%B4%9F%E9%9D%A2%E5%BD%B1%E5%93%8D\" #负面影响 3\n",
    "        ,\"https://www.douban.com/search?cat=1015&q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD+%E7%81%BE%E5%AE%B3\" #灾害\n",
    "        ,\"https://www.douban.com/search?cat=1015&q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD+%E5%9B%B0%E5%A2%83\" #困境\n",
    "        ,\"https://www.douban.com/search?cat=1015&q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD+%E6%AC%BA%E8%AF%88\" #欺诈\n",
    "        ,\"https://www.douban.com/search?cat=1015&q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD+%E6%BC%8F%E6%B4%9E\" #漏洞\n",
    "        ] \n",
    "    \n",
    "    urls = []\n",
    "\n",
    "    for base_url in base_urls:\n",
    "        urls.append(base_url)\n",
    "        prefix = \"https://www.douban.com/j/search?q=\"\n",
    "        suffix = \"&start={param}&cat=1015\"\n",
    "        middle = base_url[41:]\n",
    "        for p in range(1, 101):\n",
    "            url = prefix + middle + suffix.format(param = p * 20)\n",
    "            urls.append(url)\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "def generate_hrefs(urls):\n",
    "\n",
    "    hrefs = []\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        search_window = driver.current_window_handle\n",
    "        pageContent = driver.page_source\n",
    "        time.sleep(4)\n",
    "\n",
    "        pattern = re.compile('sid: (\\d+)', re.S)\n",
    "        results = re.findall(pattern, pageContent)\n",
    "\n",
    "        for line in results:\n",
    "            href = \"https://www.douban.com/note/\" + line\n",
    "            hrefs.append(href)\n",
    "\n",
    "    return hrefs\n",
    "\n",
    "#设置爬取的文章的时间段\n",
    "# this_year = time.strptime(\"2020-08-08 00:00:00\",\"%Y-%m-%d %H:%M:%S\")\n",
    "# this_year = \"2021-11-06 08:30:46\"\n",
    "\n",
    "\n",
    "def generate_articles(hrefs):\n",
    "    \n",
    "    print(hrefs)\n",
    "\n",
    "    articles = []\n",
    "    for href in hrefs:\n",
    "        print(href)\n",
    "\n",
    "        driver.get(href)\n",
    "\n",
    "        search_window = driver.current_window_handle\n",
    "        pageSource = driver.page_source\n",
    "        time.sleep(1)\n",
    "\n",
    "        try :\n",
    "            soup = BeautifulSoup(pageSource)\n",
    "            title_author = soup.find(\"div\", attrs={\"class\": \"note-header note-header-container\"})\n",
    "            # 文章标题\n",
    "            title = title_author.find(\"h1\", attrs={\"\": \"\"}).text\n",
    "            # 文章作者\n",
    "            author = title_author.find(\"a\", attrs={\"class\": \"note-author\"}).text\n",
    "            # 文章发布时间\n",
    "            date = title_author.find(\"span\", attrs={\"class\": \"pub-date\"}).text\n",
    "#             publish_time = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "#             print(date)\n",
    "#             if date < this_year:\n",
    "#                 continue\n",
    "\n",
    "        # 文章正文\n",
    "            res = soup.find(\"div\", attrs={\"id\": \"link-report\"})\n",
    "            note = res.find(\"div\", attrs={\"class\": \"note\"})\n",
    "            txt = note.text\n",
    "\n",
    "        except :\n",
    "            continue\n",
    "\n",
    "\n",
    "        # 点赞数\n",
    "        try:\n",
    "            support = soup.find(\"span\", attrs={\"class\": \"react-num\"}).text\n",
    "\n",
    "        except:\n",
    "            support = \"\"\n",
    "        # 标签\n",
    "        try:\n",
    "            topics = soup.find('div', attrs={\"class\": \"mod-tags\"})\n",
    "            topics_a = topics.find_all(\"a\")\n",
    "\n",
    "            tags = []\n",
    "            for topic_a in topics_a:\n",
    "                topic = topic_a.text\n",
    "                tags.append(topic)\n",
    "\n",
    "            tag = \" \".join(tags)\n",
    "\n",
    "        except:\n",
    "            tag = \"\"\n",
    "\n",
    "        # 评论信息\n",
    "        try:\n",
    "            divs = soup.find_all(\"div\", attrs={\"class\": \"comment-content\"})\n",
    "            meta_headers = soup.find_all(\"div\", attrs={\"class\": \"meta-header\"})\n",
    "\n",
    "            comments = []\n",
    "            for line1, line2 in zip(divs, meta_headers):\n",
    "                span = line1.find(\"span\")\n",
    "                review = span.text\n",
    "                a = line2.find_all(\"a\", attrs={\"title\": re.compile(\".*\")})\n",
    "                comment_user = a[0].text\n",
    "                comment_time = line2.find(\"time\").text\n",
    "                comment = [comment_user, review, comment_time]\n",
    "                comments.append(comment)\n",
    "\n",
    "        except:\n",
    "            comments = []\n",
    "\n",
    "        article = [href, title, author, date, txt, support, tag, comments]\n",
    "        print(article)\n",
    "        articles.append(article)\n",
    "    \n",
    "    print(articles)\n",
    "    return articles\n",
    "\n",
    "def write_csv(datalist, path):\n",
    "    headers=[\"link\",\"title\",\"author\",\"date\",\"text\",\"vote\",\"label\",\"comment\"]\n",
    "    test = pd.DataFrame(columns=headers,data=datalist)\n",
    "    test.to_csv(path, encoding='utf-8-sig')\n",
    "    return test\n",
    "\n",
    "\n",
    "urls = generate_urls()\n",
    "hrefs = generate_hrefs(urls)\n",
    "articles = generate_articles(hrefs)\n",
    "driver.quit()\n",
    "path = r'./douban.csv'\n",
    "\n",
    "write_csv(articles, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "douban.to_csv('douban.csv', encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
